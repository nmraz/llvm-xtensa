; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=xtensa | FileCheck %s
; RUN: llc < %s -mtriple=xtensa -mcpu=lx7 | FileCheck %s --check-prefix=LX7

declare i64 @llvm.smin.i64(i64 %a, i64 %b)
declare i64 @llvm.umin.i64(i64 %a, i64 %b)
declare i64 @llvm.smax.i64(i64 %a, i64 %b)
declare i64 @llvm.umax.i64(i64 %a, i64 %b)

define i64 @smin_i64(i64 %a, i64 %b) {
; CHECK-LABEL: smin_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movi.n a9, 0
; CHECK-NEXT:    sub a7, a3, a5
; CHECK-NEXT:    movi.n a6, 1
; CHECK-NEXT:    movi.n a8, 1
; CHECK-NEXT:    movgez a6, a9, a7
; CHECK-NEXT:    bltu a2, a4, .LBB0_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    movi.n a8, 0
; CHECK-NEXT:  .LBB0_2:
; CHECK-NEXT:    moveqz a6, a8, a7
; CHECK-NEXT:    moveqz a2, a4, a6
; CHECK-NEXT:    moveqz a3, a5, a6
; CHECK-NEXT:    ret.n
;
; LX7-LABEL: smin_i64:
; LX7:       # %bb.0:
; LX7-NEXT:    salt a6, a3, a5
; LX7-NEXT:    saltu a7, a2, a4
; LX7-NEXT:    sub a8, a3, a5
; LX7-NEXT:    movnez a7, a6, a8
; LX7-NEXT:    moveqz a2, a4, a7
; LX7-NEXT:    moveqz a3, a5, a7
; LX7-NEXT:    ret.n
  %smin = call i64 @llvm.smin.i64(i64 %a, i64 %b)
  ret i64 %smin
}

define i64 @umin_i64(i64 %a, i64 %b) {
; CHECK-LABEL: umin_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movi.n a7, 1
; CHECK-NEXT:    movi.n a6, 1
; CHECK-NEXT:    bltu a3, a5, .LBB1_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    movi.n a6, 0
; CHECK-NEXT:  .LBB1_2:
; CHECK-NEXT:    bltu a2, a4, .LBB1_4
; CHECK-NEXT:  # %bb.3:
; CHECK-NEXT:    movi.n a7, 0
; CHECK-NEXT:  .LBB1_4:
; CHECK-NEXT:    sub a8, a3, a5
; CHECK-NEXT:    moveqz a6, a7, a8
; CHECK-NEXT:    moveqz a2, a4, a6
; CHECK-NEXT:    moveqz a3, a5, a6
; CHECK-NEXT:    ret.n
;
; LX7-LABEL: umin_i64:
; LX7:       # %bb.0:
; LX7-NEXT:    saltu a6, a3, a5
; LX7-NEXT:    saltu a7, a2, a4
; LX7-NEXT:    sub a8, a3, a5
; LX7-NEXT:    movnez a7, a6, a8
; LX7-NEXT:    moveqz a2, a4, a7
; LX7-NEXT:    moveqz a3, a5, a7
; LX7-NEXT:    ret.n
  %umin = call i64 @llvm.umin.i64(i64 %a, i64 %b)
  ret i64 %umin
}

define i64 @smax_i64(i64 %a, i64 %b) {
; CHECK-LABEL: smax_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movi.n a8, 0
; CHECK-NEXT:    sub a9, a5, a3
; CHECK-NEXT:    movi.n a6, 1
; CHECK-NEXT:    movi.n a7, 1
; CHECK-NEXT:    movgez a6, a8, a9
; CHECK-NEXT:    bltu a4, a2, .LBB2_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    movi.n a7, 0
; CHECK-NEXT:  .LBB2_2:
; CHECK-NEXT:    sub a8, a3, a5
; CHECK-NEXT:    moveqz a6, a7, a8
; CHECK-NEXT:    moveqz a2, a4, a6
; CHECK-NEXT:    moveqz a3, a5, a6
; CHECK-NEXT:    ret.n
;
; LX7-LABEL: smax_i64:
; LX7:       # %bb.0:
; LX7-NEXT:    salt a6, a5, a3
; LX7-NEXT:    saltu a7, a4, a2
; LX7-NEXT:    sub a8, a3, a5
; LX7-NEXT:    movnez a7, a6, a8
; LX7-NEXT:    moveqz a2, a4, a7
; LX7-NEXT:    moveqz a3, a5, a7
; LX7-NEXT:    ret.n
  %smax = call i64 @llvm.smax.i64(i64 %a, i64 %b)
  ret i64 %smax
}

define i64 @umax_i64(i64 %a, i64 %b) {
; CHECK-LABEL: umax_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movi.n a7, 1
; CHECK-NEXT:    movi.n a6, 1
; CHECK-NEXT:    bltu a5, a3, .LBB3_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    movi.n a6, 0
; CHECK-NEXT:  .LBB3_2:
; CHECK-NEXT:    bltu a4, a2, .LBB3_4
; CHECK-NEXT:  # %bb.3:
; CHECK-NEXT:    movi.n a7, 0
; CHECK-NEXT:  .LBB3_4:
; CHECK-NEXT:    sub a8, a3, a5
; CHECK-NEXT:    moveqz a6, a7, a8
; CHECK-NEXT:    moveqz a2, a4, a6
; CHECK-NEXT:    moveqz a3, a5, a6
; CHECK-NEXT:    ret.n
;
; LX7-LABEL: umax_i64:
; LX7:       # %bb.0:
; LX7-NEXT:    saltu a6, a5, a3
; LX7-NEXT:    saltu a7, a4, a2
; LX7-NEXT:    sub a8, a3, a5
; LX7-NEXT:    movnez a7, a6, a8
; LX7-NEXT:    moveqz a2, a4, a7
; LX7-NEXT:    moveqz a3, a5, a7
; LX7-NEXT:    ret.n
  %umax = call i64 @llvm.umax.i64(i64 %a, i64 %b)
  ret i64 %umax
}
