; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=xtensa | FileCheck %s

declare void @reg_args_i32(i32 %arg1, i32 %arg2, i32 %arg3, i32 %arg4, i32 %arg5, i32 %arg6)
declare void @reg_arg_i1(i1 %arg1)
declare void @reg_arg_i8(i8 %arg1)
declare void @reg_arg_i16(i16 %arg1)
declare void @reg_arg_i64(i64 %arg1)
declare void @reg_arg_i64_packed(i32 %arg1, i32 %arg2, i64 %arg3)
declare void @reg_arg_i64_aligned(i32 %arg1, i64 %arg2)
declare void @mixed_reg_args(i32 %arg1, i1 %arg2, i64 %arg3, i8 %arg4, i16 %arg5)
declare void @stack_arg_i32(i32 %arg1, i32 %arg2, i32 %arg3, i32 %arg4, i32 %arg5, i32 %arg6, i32 %arg7)
declare void @stack_arg_i1(i32 %arg1, i32 %arg2, i32 %arg3, i32 %arg4, i32 %arg5, i32 %arg6, i1 %arg7)
declare void @stack_arg_i8(i32 %arg1, i32 %arg2, i32 %arg3, i32 %arg4, i32 %arg5, i32 %arg6, i8 %arg7)
declare void @stack_arg_i16(i32 %arg1, i32 %arg2, i32 %arg3, i32 %arg4, i32 %arg5, i32 %arg6, i16 %arg7)
declare void @stack_arg_i64(i32 %arg1, i32 %arg2, i32 %arg3, i32 %arg4, i32 %arg5, i32 %arg6, i64 %arg7)
declare void @stack_arg_i64_reg_unaligned(i32 %arg1, i32 %arg2, i32 %arg3, i32 %arg4, i32 %arg5, i64 %arg6)
declare void @stack_arg_i64_aligned(i32 %arg1, i32 %arg2, i32 %arg3, i32 %arg4, i32 %arg5, i32 %arg6, i32 %arg7, i64 %arg8)
declare void @mixed_reg_stack_args(i32 %arg1, i1 %arg2, i8 %arg3, i64 %arg4, i16 %arg5)
declare void @mixed_stack_args(i32 %arg1, i32 %arg2, i32 %arg3, i32 %arg4, i32 %arg5, i32 %arg6, i16 %arg7, i8 %arg8, i64 %arg9)

define void @call_simple_reg_args() {
; CHECK-LABEL: call_simple_reg_args:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi a1, a1, -16
; CHECK-NEXT:    addi a2, a1, 12
; CHECK-NEXT:    s32i a0, a2, 0 # 4-byte Folded Spill
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a3, 2
; CHECK-NEXT:    movi.n a4, 3
; CHECK-NEXT:    movi.n a5, 4
; CHECK-NEXT:    movi.n a6, 5
; CHECK-NEXT:    movi.n a7, 6
; CHECK-NEXT:    call0 reg_args_i32
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    call0 reg_arg_i1
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    call0 reg_arg_i8
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    call0 reg_arg_i16
; CHECK-NEXT:    addi a2, a1, 12
; CHECK-NEXT:    l32i a0, a2, 0 # 4-byte Folded Reload
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    ret.n
entry:
  call void @reg_args_i32(i32 1, i32 2, i32 3, i32 4, i32 5, i32 6)
  call void @reg_arg_i1(i1 1)
  call void @reg_arg_i8(i8 1)
  call void @reg_arg_i16(i16 1)
  ret void
}

define void @call_reg_i64() {
; CHECK-LABEL: call_reg_i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi a1, a1, -16
; CHECK-NEXT:    addi a2, a1, 12
; CHECK-NEXT:    s32i a0, a2, 0 # 4-byte Folded Spill
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a3, 0
; CHECK-NEXT:    call0 reg_arg_i64
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a3, 2
; CHECK-NEXT:    movi.n a4, 3
; CHECK-NEXT:    movi.n a5, 0
; CHECK-NEXT:    call0 reg_arg_i64_packed
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a4, 2
; CHECK-NEXT:    movi.n a5, 0
; CHECK-NEXT:    call0 reg_arg_i64_aligned
; CHECK-NEXT:    addi a2, a1, 12
; CHECK-NEXT:    l32i a0, a2, 0 # 4-byte Folded Reload
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    ret.n
entry:
  call void @reg_arg_i64(i64 1)
  call void @reg_arg_i64_packed(i32 1, i32 2, i64 3)
  call void @reg_arg_i64_aligned(i32 1, i64 2)
  ret void
}

define void @call_mixed_reg_args() {
; CHECK-LABEL: call_mixed_reg_args:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi a1, a1, -16
; CHECK-NEXT:    addi a2, a1, 12
; CHECK-NEXT:    s32i a0, a2, 0 # 4-byte Folded Spill
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a3, 1
; CHECK-NEXT:    movi.n a4, 3
; CHECK-NEXT:    movi.n a5, 0
; CHECK-NEXT:    movi.n a6, 4
; CHECK-NEXT:    movi.n a7, 5
; CHECK-NEXT:    call0 mixed_reg_args
; CHECK-NEXT:    addi a2, a1, 12
; CHECK-NEXT:    l32i a0, a2, 0 # 4-byte Folded Reload
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    ret.n
entry:
  call void @mixed_reg_args(i32 1, i1 1, i64 3, i8 4, i16 5)
  ret void
}

define void @call_stack() {
; CHECK-LABEL: call_stack:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi a1, a1, -16
; CHECK-NEXT:    addi a2, a1, 12
; CHECK-NEXT:    s32i a0, a2, 0 # 4-byte Folded Spill
; CHECK-NEXT:    addi a2, a1, 8
; CHECK-NEXT:    s32i a12, a2, 0 # 4-byte Folded Spill
; CHECK-NEXT:    addi a2, a1, 4
; CHECK-NEXT:    s32i a13, a2, 0 # 4-byte Folded Spill
; CHECK-NEXT:    movi.n a12, 7
; CHECK-NEXT:    s32i.n a12, a1, 0
; CHECK-NEXT:    movi.n a13, 1
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a3, 2
; CHECK-NEXT:    movi.n a4, 3
; CHECK-NEXT:    movi.n a5, 4
; CHECK-NEXT:    movi.n a6, 5
; CHECK-NEXT:    movi.n a7, 6
; CHECK-NEXT:    call0 stack_arg_i32
; CHECK-NEXT:    s32i.n a13, a1, 0
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a3, 2
; CHECK-NEXT:    movi.n a4, 3
; CHECK-NEXT:    movi.n a5, 4
; CHECK-NEXT:    movi.n a6, 5
; CHECK-NEXT:    movi.n a7, 6
; CHECK-NEXT:    call0 stack_arg_i1
; CHECK-NEXT:    s32i.n a12, a1, 0
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a3, 2
; CHECK-NEXT:    movi.n a4, 3
; CHECK-NEXT:    movi.n a5, 4
; CHECK-NEXT:    movi.n a6, 5
; CHECK-NEXT:    movi.n a7, 6
; CHECK-NEXT:    call0 stack_arg_i8
; CHECK-NEXT:    s32i.n a12, a1, 0
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a3, 2
; CHECK-NEXT:    movi.n a4, 3
; CHECK-NEXT:    movi.n a5, 4
; CHECK-NEXT:    movi.n a6, 5
; CHECK-NEXT:    movi.n a7, 6
; CHECK-NEXT:    call0 stack_arg_i16
; CHECK-NEXT:    addi a2, a1, 4
; CHECK-NEXT:    l32i a13, a2, 0 # 4-byte Folded Reload
; CHECK-NEXT:    addi a2, a1, 8
; CHECK-NEXT:    l32i a12, a2, 0 # 4-byte Folded Reload
; CHECK-NEXT:    addi a2, a1, 12
; CHECK-NEXT:    l32i a0, a2, 0 # 4-byte Folded Reload
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    ret.n
entry:
  call void @stack_arg_i32(i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7)
  call void @stack_arg_i1(i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i1 7)
  call void @stack_arg_i8(i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i8 7)
  call void @stack_arg_i16(i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i16 7)
  ret void
}

define void @call_stack_i64() {
; CHECK-LABEL: call_stack_i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi a1, a1, -16
; CHECK-NEXT:    addi a2, a1, 12
; CHECK-NEXT:    s32i a0, a2, 0 # 4-byte Folded Spill
; CHECK-NEXT:    addi a2, a1, 8
; CHECK-NEXT:    s32i a12, a2, 0 # 4-byte Folded Spill
; CHECK-NEXT:    addi a2, a1, 4
; CHECK-NEXT:    s32i a13, a2, 0 # 4-byte Folded Spill
; CHECK-NEXT:    addi a2, a1, 0
; CHECK-NEXT:    s32i a14, a2, 0 # 4-byte Folded Spill
; CHECK-NEXT:    movi.n a13, 7
; CHECK-NEXT:    s32i.n a13, a1, 0
; CHECK-NEXT:    movi.n a12, 0
; CHECK-NEXT:    s32i.n a12, a1, 4
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a3, 2
; CHECK-NEXT:    movi.n a4, 3
; CHECK-NEXT:    movi.n a5, 4
; CHECK-NEXT:    movi.n a6, 5
; CHECK-NEXT:    movi.n a14, 6
; CHECK-NEXT:    movi.n a7, 6
; CHECK-NEXT:    call0 stack_arg_i64
; CHECK-NEXT:    s32i.n a14, a1, 0
; CHECK-NEXT:    s32i.n a12, a1, 4
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a3, 2
; CHECK-NEXT:    movi.n a4, 3
; CHECK-NEXT:    movi.n a5, 4
; CHECK-NEXT:    movi.n a6, 5
; CHECK-NEXT:    call0 stack_arg_i64_reg_unaligned
; CHECK-NEXT:    s32i.n a13, a1, 0
; CHECK-NEXT:    movi.n a2, 8
; CHECK-NEXT:    s32i.n a2, a1, 8
; CHECK-NEXT:    s32i.n a12, a1, 12
; CHECK-NEXT:    movi.n a2, 1
; CHECK-NEXT:    movi.n a3, 2
; CHECK-NEXT:    movi.n a4, 3
; CHECK-NEXT:    movi.n a5, 4
; CHECK-NEXT:    movi.n a6, 5
; CHECK-NEXT:    movi.n a7, 6
; CHECK-NEXT:    call0 stack_arg_i64_aligned
; CHECK-NEXT:    addi a2, a1, 0
; CHECK-NEXT:    l32i a14, a2, 0 # 4-byte Folded Reload
; CHECK-NEXT:    addi a2, a1, 4
; CHECK-NEXT:    l32i a13, a2, 0 # 4-byte Folded Reload
; CHECK-NEXT:    addi a2, a1, 8
; CHECK-NEXT:    l32i a12, a2, 0 # 4-byte Folded Reload
; CHECK-NEXT:    addi a2, a1, 12
; CHECK-NEXT:    l32i a0, a2, 0 # 4-byte Folded Reload
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    ret.n
entry:
  call void @stack_arg_i64(i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i64 7)
  call void @stack_arg_i64_reg_unaligned(i32 1, i32 2, i32 3, i32 4, i32 5, i64 6)
  call void @stack_arg_i64_aligned(i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i64 8)
  ret void
}
